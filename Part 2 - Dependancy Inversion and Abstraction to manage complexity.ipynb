{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2: Dependency Inversion/Abstraction to manage complexity and enable POCs to scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This is the most critical take-away from the workshop today so I will spend the bulk of our 2 hours on it, if this causes us to miss the later material, feel free to refer to it after the workshop on your own and reach out via slack if you have any questions. To start with lets talk about dependacies. If you are new to the software world you might be asking yourself: What is a dependancy? Simply put, it is a relationship between two parts of your pipeline/program. Your pipeline will accumulate dependancies as you work. These dependancies could be third party libraries outside your control like Pandas, it could be certain files or databases, one part of your code could depend on another part, your code could become dependant on a certain visualization software (spotfire, excel, etc), and there are many more potential examples. \n",
    "\n",
    "    If you were to isolate one part of your pipeline (let's say it is some code for this example). You can imagine looking backwards in your pipeline and everything backwards would be dependencies of this bit of code (i.e. this code depends on *x*). You could also imagine looking foward in your pipeline, these would be the things that *depend* on this particular bit of code. Let's now introduce the idea of code **stability**. The more things that depend on a particular bit of code, the more stable that code must be. Why? Imagine a bit of code with only one dependancy. If we make a change to this code, we only have to consider the impact of that change on one dependency downstream of this code. This makes changes to the code cheap, so there is low stability/high agility for this code. Now imagine you had 30 dependencies on this piece of code instead. This code would have to be very stable, because any change is enormously costly! You have to account for changes across a huge range of dependencies. \n",
    "    \n",
    "    This relationship between agility/stability is at the core of why data projects that started as small POCs grow unmanageable with scale. Luckily, there is a strategy for managing this dynamic: dependency inversion (also called abstraction). Let's see a simple example of this in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1\n",
      "B2\n",
      "C2\n",
      "A1\n",
      "B2\n",
      "D1\n"
     ]
    }
   ],
   "source": [
    "# Let's say we have geologic pick data for a POC data science project on a small field\n",
    "import pandas as pd\n",
    "\n",
    "# Example Pick Data\n",
    "poc_picks_df = pd.DataFrame({\n",
    "    'pick_name': ['A1', 'B2', 'C2', 'A1', 'B2', 'D1'],\n",
    "    'pick_depth': [100.0, 150.0, 400.0, 300.0, 400.0, 800.0],\n",
    "    'pick_interpreter': ['AJ', 'AJ', 'AJ', 'AJ', 'AJ', 'AJ'],\n",
    "    'well': ['00000000010000', '00000000010000', '00000000010000', '00000000020000', '00000000020000', '00000000020000']\n",
    "})\n",
    "\n",
    "# Our project pipeline (imagine this is 2000 lines long and very awesome/complicated)\n",
    "def project_pipeline(picks_df):\n",
    "    # Imagine a long, complicated pipeline for the rest of the function!!\n",
    "    for idx, row in picks_df.iterrows():\n",
    "        # We are just printing the pick name to keep the actual example really simple\n",
    "        print(row['pick_name'])\n",
    "    return 'awesome pipeline outputs'\n",
    "\n",
    "# Alright let's run our 'POC' pipeline and see what happens!\n",
    "pipeline_output = project_pipeline(poc_picks_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, our POC is working great, the data runs through our 'pipeline' and returns outputs. The higher-ups are excited at the results and the inevitable requests comes: 'How soon can you run this for the whole company?'. Let's see what happens for our example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pick_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/lambda-oil-and-gas/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pick_name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ff7090715b89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Thinking that your big, well organized company had excellent controls over the data, you deploy your massive,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# complex pipeline on this grand new dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpipeline_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpit_of_despair_picks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-332ce7a1a09b>\u001b[0m in \u001b[0;36mproject_pipeline\u001b[0;34m(picks_df)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpicks_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# We are just printing the pick name to keep the actual example really simple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pick_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m'awesome pipeline outputs'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/lambda-oil-and-gas/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/lambda-oil-and-gas/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/lambda-oil-and-gas/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pick_name'"
     ]
    }
   ],
   "source": [
    "# The boss suggests we try our pipeline on an up-and-coming asset called 'The pit of despair' what a strange name! \n",
    "# They send you the info to get their picks:\n",
    "\n",
    "pit_of_despair_picks = pd.DataFrame({\n",
    "    'PICK_SURF_NAME': ['a_1', None, 'AA2', 'bb1', 'AA2'],\n",
    "    'MD': [100.0, 200.0, 300.0, 100.0, 600.0],\n",
    "    'GUY WHO PICKED IT': ['BOB', 'SATAN', 'BOB', 'BOB', 'BOB'],\n",
    "    'WellName': ['HADES1', 'HADIES2', 'HADES2', 'HADES3', 'HADES3'],\n",
    "    'API': ['0000000010', '0000000011', '0000000011', '0000000012', '0000000012']\n",
    "})\n",
    "\n",
    "# Thinking that your big, well organized company had excellent controls over the data, you deploy your massive, \n",
    "# complex pipeline on this grand new dataset.\n",
    "pipeline_output = project_pipeline(pit_of_despair_picks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh my! Your pipelines has failed because the schema of the original POC does not match the new data! Now in reality our pipeline is tiny, and a small tweak to the data could fix this, but what if the pipeline were truly large and 1000's of lines long? What if we were in a company with hundreds of assets each with their own quirks and differences to how picks are managed? How can we prevent this **dependency** on the POC's database schema from bogging down our scaling up to the whole company? The solution? Let's invert the dependency using an abstraction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create an abstract class called a validator\n",
    "class Validator(object):\n",
    "    # This class will define two methods that all validators should implement:\n",
    "    def check_if_valid(self, df)->bool:\n",
    "        # This method will be used to quickly check if the validator needs to run\n",
    "        return True\n",
    "    def validate(self, df)->pd.DataFrame:\n",
    "        # This method will attempt to automatically validate an input dataframe and return the result\n",
    "        # Because this is an abstract class, we want to raise an error if it is called directly!\n",
    "        raise NotImplementedError('Do not invoke abstract method directly')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we do above? We created an abstract class or simply an abstraction. What **is** an abstraction? Think of it as communicating an idea instead of instructions for how to do something specifically. In this case, we are conveying the **idea** that all validators should implement a function to check the validity of data and a function to attempt to perform an automatic validation on the data. Let's make a specific **implementation** of this **idea** to fix our column naming problem above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We creating a class that is a child of our abstract class\n",
    "class ColumnNameValidation(Validator):\n",
    "    column_names = {\n",
    "        'PICK_SURF_NAME': 'pick_name',\n",
    "        'MD': 'pick_depth',\n",
    "        'GUY WHO PICKED IT': 'pick_interpreter',\n",
    "        'API': 'well'\n",
    "    }\n",
    "    # First we implement the first function (in an overly simple way for this example)\n",
    "    def check_if_valid(self, df)->bool:\n",
    "        # If pick name is not in the database, we know our pipeline will fail so lets focus on that in this example\n",
    "        if 'pick_name' not in df:\n",
    "            # Tell the pipeline we need to validate the dataframe\n",
    "            return False\n",
    "        # If we get here, assume the df is ok\n",
    "        return True\n",
    "    \n",
    "    # Next up, actually validating the dataframe if we can\n",
    "    def validate(self, df)->pd.DataFrame:\n",
    "        df = df.rename(columns=self.column_names)\n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_1\n",
      "None\n",
      "AA2\n",
      "bb1\n",
      "AA2\n",
      "A1\n",
      "B2\n",
      "C2\n",
      "A1\n",
      "B2\n",
      "D1\n"
     ]
    }
   ],
   "source": [
    "# Let's update the project pipeline\n",
    "# Our project pipeline (imagine this is 2000 lines long and very awesome/complicated)\n",
    "def project_pipeline(picks_df):\n",
    "    #*** NEW *** Let's put our validation process here\n",
    "    validators = [ColumnNameValidation()]\n",
    "    for validator in validators:\n",
    "        if not validator.check_if_valid(picks_df):\n",
    "            picks_df = validator.validate(picks_df)\n",
    "        \n",
    "    # Imagine a long, complicated pipeline for the rest of the function!!\n",
    "    for idx, row in picks_df.iterrows():\n",
    "        # We are just printing the pick name to keep the actual example really simple\n",
    "        print(row['pick_name'])\n",
    "    return 'awesome pipeline outputs'\n",
    "\n",
    "# Now re-run the 'pit of despair'\n",
    "pipeline_output = project_pipeline(pit_of_despair_picks)\n",
    "\n",
    "# And run the old pipeline to check it still works\n",
    "pipeline_output = project_pipeline(poc_picks_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so it works on both assets now! If you are thinking, hey that seems really complicated for just changing some column names, then you would be right! However, think about this in terms a larger, more complex pipeline. There are two points where you are using abstractions, the 1st is when you **created an abstraction of what all the picks should look like after validation for the entire pipeline**. No matter how the picks looked originally, in **your** pipeline they all look exactly the same post validation. This is actually pretty huge because your pipeline **no longer has a direct dependency on any of the database/file schemas.** Your pipeline now depends on an *idea*, an abstraction of what picks should look like. \n",
    "\n",
    "The second abstraction was the creation of an abstraction of a validator. We will talk more about this in part 2, but notice that your pipeline has essentially an **unlimited capacity to add new implementations of validators**. We created one to handle columns, but we could create type, well name, or other types of validations and add them to our pipeline at any time and our pipeline won't care one bit about it. Why? because the pipeline only **depends on the abstraction validator and not any any specific implementation of the validator**. As long as the validator has a check_if_valid method and a validate method, our pipeline can run it **without knowing a single thing about how it is implemented**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The power of a dependency inversion\n",
    "\n",
    "Imagine a pipeline that goes from start to finish with no effort at abstraction/inverting dependencies. A failure or change anywhere in the chain will ripple through the pipeline and cause failures, technical debt, and above all **stability** or resistance to change. Code with too many dependancies can't change easily and when **forced** to change, causes a lot of technical debt and suffering your part. Abstraction can free you from this dynamic because it allows you to *isolate* changes to one part of your code base. In our pick example above, changes to the database schema are isolated to the region below the abstaction. A new asset or schema **will never require any changes to your pipeline post the column validation/abstraction**. One place for the changes to happen is WAY better than 10s or 100s of places to handle those type of changes. Let's look at another example with 3rd party libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a abstraction for machine learning for wrapping sklearn\n",
    "class MachineLearningModel(object):\n",
    "    # For our simple example let's define two methods for our abstraction:\n",
    "    def fit(self, x, y):\n",
    "        pass\n",
    "    def predict(self, x):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we have an abstraction, now how should we use it, let's try 'wrapping' a linear model from sk_learn:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.05007685,  0.33744006,  0.0069527 ,  0.05596978,  0.63257931,\n",
       "        0.49220713,  0.03034089,  0.32403152,  1.03415888,  1.08163055,\n",
       "        0.24723058,  0.71496601,  0.32731097,  0.05376386, -0.08754283,\n",
       "        0.42260194,  0.91864455,  0.62007031,  0.99488139,  0.21374181,\n",
       "        0.36742069, -0.09005192,  0.78922337,  0.17582194,  0.82134264,\n",
       "        1.07335718, -0.05007685,  0.21077663,  1.06342366,  1.08720309,\n",
       "        0.94403022,  0.78625819,  0.45003852,  1.06241699,  0.59161401,\n",
       "        0.97168328,  0.67268903,  1.04885494,  0.28822164,  0.70469236,\n",
       "        0.37992969,  0.40838599,  0.11097812, -0.03272674,  0.08135545,\n",
       "        0.85748986,  0.96965911,  0.25031333,  0.88902188,  0.46528146,\n",
       "       -0.08219522, -0.07414447,  0.71177836,  1.08754283,  0.63593709,\n",
       "       -0.04885494,  0.50779287, -0.01573515,  1.01409868,  0.17865736,\n",
       "        0.08373407,  0.54996148,  0.02831672,  1.07414447,  0.59503633,\n",
       "        0.88648034,  1.09016532,  0.94623614, -0.06342366,  0.74629442,\n",
       "        0.91626593,  0.53471854,  1.09005192,  0.14251014,  0.67596848,\n",
       "        0.75276942, -0.03415888,  0.28503399, -0.06241699,  0.40496367,\n",
       "        1.08219522, -0.08163055,  0.9930473 , -0.07335718,  1.03272674,\n",
       "       -0.09016532, -0.08720309,  0.57739806,  0.74968667,  0.85479584,\n",
       "        0.36406291,  0.82417806,  0.25370558,  0.66255994,  0.29530764,\n",
       "       -0.01409868,  0.11351966,  0.00511861,  1.01573515,  0.14520416])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "class SkLearnWrappedModel(MachineLearningModel):\n",
    "    def __init__(self):\n",
    "        self.model = LinearRegression()\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.model.fit(x, y)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "\n",
    "## Let's create an example dataset to try:\n",
    "dataset, y = make_moons(100)\n",
    "\n",
    "model = SkLearnWrappedModel()\n",
    "# Fit the dataset\n",
    "model.fit(dataset, y)\n",
    "# Predict the dataset\n",
    "predictions = model.predict(dataset)\n",
    "predictions\n",
    "# Note this is to illustrate abstraction not good data science practices \n",
    "# (note the lack of test/train split, cross validation, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that might seem kind of a dumb thing to do right? The developers of sci-kit learn put so much effort into designing a nice API and you go a make your own to put around theirs? Why do that? Well, let's explore the benefits of building a 'wrapper' type of abstaction/dependency inversion:\n",
    "- 1: It isolates the change from the third party library to one location in your code. \n",
    "- 2: It abstracts away the api of the library to one *you* control and that allows you to use other similar packages interchangeablely in your pipeline!\n",
    "\n",
    "This is actually pretty huge! Imagine you did NOT do this and the library releases a new version that breaks your pipeline. This could mean hunting down dozens or hundreds of references to this library in your pipeline to make your code compatible with the changes. If you do this though, you only have to go to one place to update your code to be compatible... The best part? It doesn't cost you much time at all if you do it early! \n",
    "\n",
    "When it comes to abstraction there is not really a speed vs quality trade off, it is just are you going to setup your abstractions early when it is cheap to do or are you going to wait until it is a big, unpleasant job of not do it at all and watch the complexity and technical debt kill the project??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
