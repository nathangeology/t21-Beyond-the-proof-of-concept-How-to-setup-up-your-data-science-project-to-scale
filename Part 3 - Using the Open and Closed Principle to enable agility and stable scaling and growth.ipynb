{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 3: Open/Closed Principle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the workshop focuses on the open/closed principle: \"Code should be open for *extension* but closed for *modification*.\" This principle is very important for projects that need to keep technical debt minimized and deployed portions of the pipeline working despite ongoing development. Applying this principle effectively in data science leads to true agility and rapid adaptation to new requirements and changes. Let's go back to the example from part 2 looking at open/closed principle as applied to the 'validator' abstraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Let's return to our simple example \"pipeline\" from part 2:\n",
    "def project_pipeline(picks_df):\n",
    "    # We are going to focus here for this part of the workshop:\n",
    "    validators = [ColumnNameValidation()]\n",
    "    for validator in validators:\n",
    "        if not validator.check_if_valid(picks_df):\n",
    "            picks_df = validator.validate(picks_df)\n",
    "        \n",
    "    # Imagine a long, complicated pipeline for the rest of the function!!\n",
    "    for idx, row in picks_df.iterrows():\n",
    "        # We are just printing the pick name to keep the actual example really simple\n",
    "        print(row['pick_name'])\n",
    "    return 'awesome pipeline outputs'\n",
    "\n",
    "\n",
    "# Example Picks data \n",
    "poc_picks_df = pd.DataFrame({\n",
    "    'pick_name': ['A1', 'B2', 'C2', 'A1', 'B2', 'D1'],\n",
    "    'pick_depth': [100.0, 150.0, 400.0, 300.0, 400.0, 800.0],\n",
    "    'pick_interpreter': ['AJ', 'AJ', 'AJ', 'AJ', 'AJ', 'AJ'],\n",
    "    'well': ['00000000010000', '00000000010000', '00000000010000', '00000000020000', '00000000020000', '00000000020000']\n",
    "})\n",
    "pit_of_despair_picks = pd.DataFrame({\n",
    "    'PICK_SURF_NAME': ['a_1', None, 'AA2', 'bb1', 'AA2'],\n",
    "    'MD': [100.0, 200.0, 300.0, 100.0, 600.0],\n",
    "    'GUY WHO PICKED IT': ['BOB', 'SATAN', 'BOB', 'BOB', 'BOB'],\n",
    "    'WellName': ['HADES1', 'HADIES2', 'HADES2', 'HADES3', 'HADES3'],\n",
    "    'API': ['0000000010', '0000000011', '0000000011', '0000000012', '0000000012']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add in the validator class and the implementation we did from last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create an abstract class called a validator\n",
    "class Validator(object):\n",
    "    # This class will define two methods that all validators should implement:\n",
    "    def check_if_valid(self, df)->bool:\n",
    "        # This method will be used to quickly check if the validator needs to run\n",
    "        return True\n",
    "    def validate(self, df)->pd.DataFrame:\n",
    "        # This method will attempt to automatically validate an input dataframe and return the result\n",
    "        # Because this is an abstract class, we want to raise an error if it is called directly!\n",
    "        raise NotImplementedError('Do not invoke abstract method directly')\n",
    "        \n",
    "# We creating a class that is a child of our abstract class\n",
    "class ColumnNameValidation(Validator):\n",
    "    column_names = {\n",
    "        'PICK_SURF_NAME': 'pick_name',\n",
    "        'MD': 'pick_depth',\n",
    "        'GUY WHO PICKED IT': 'pick_interpreter',\n",
    "        'API': 'well'\n",
    "    }\n",
    "    # First we implement the first function (in an overly simple way for this example)\n",
    "    def check_if_valid(self, df)->bool:\n",
    "        # If pick name is not in the database, we know our pipeline will fail so lets focus on that in this example\n",
    "        if 'pick_name' not in df:\n",
    "            # Tell the pipeline we need to validate the dataframe\n",
    "            return False\n",
    "        # If we get here, assume the df is ok\n",
    "        return True\n",
    "    \n",
    "    # Next up, actually validating the dataframe if we can\n",
    "    def validate(self, df)->pd.DataFrame:\n",
    "        df = df.rename(columns=self.column_names)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that our pipeline functions before we begin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting pit of despair\n",
      "a_1\n",
      "None\n",
      "AA2\n",
      "bb1\n",
      "AA2\n",
      "starting POC\n",
      "A1\n",
      "B2\n",
      "C2\n",
      "A1\n",
      "B2\n",
      "D1\n"
     ]
    }
   ],
   "source": [
    "# 'pit of despair' asset\n",
    "print('starting pit of despair')\n",
    "pipeline_output = project_pipeline(pit_of_despair_picks)\n",
    "print('starting POC')\n",
    "# poc asset\n",
    "pipeline_output = project_pipeline(poc_picks_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now we begin!\n",
    "\n",
    "Your management loves the pipeline and the speed with which you can add new assets is astonishing, but there is a problem... One of your 'customers' needs you to process your well API numbers as integers instead of strings! What should we do if we make this change to directly on our data our current pipeline could break! Let's apply the idea that our code should be closed to *modification* but open to extension, so step one is extend our validators with a new implementation that makes this change for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# This is a simple (and a bit absurd!) implementation of validator to change the target column to an integer\n",
    "class ApiToIntegerValidator(Validator):\n",
    "    def check_if_valid(self, df)->bool:\n",
    "        # Determine if the wells column is an integer\n",
    "        integer_cols_df = df.loc[:, df.dtypes <= np.integer]\n",
    "        if 'well' in integer_cols_df: # this validator could be made more flexible here, but to keep things simple, I did not\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def validate(self, df)->pd.DataFrame:\n",
    "        # This method will attempt to automatically validate an input dataframe and return the result\n",
    "        # Because this is an abstract class, we want to raise an error if it is called directly!\n",
    "        wells_col = df['well'].astype(int)\n",
    "        df['well'] = wells_col\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok we wrote a new implementation of validator (an extension in this case) to do what the customer asked of us, now we need to make our pipeline work for both the old pipeline and the new, modified one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create an idea of what we want our pipelines to do via an abstraction\n",
    "class AbstractPipeline(object):\n",
    "    def __init__(self):\n",
    "        self.df = None\n",
    "        self.validators = []\n",
    "        \n",
    "    def run(self, df):\n",
    "        self.df = df\n",
    "        self.validate_dataset()\n",
    "        return self.pipeline_function()\n",
    "    \n",
    "    def validate_dataset(self):\n",
    "        for validator in self.validators:\n",
    "            if not validator.check_if_valid(self.df):\n",
    "                self.df = validator.validate(self.df)\n",
    "    \n",
    "    def pipeline_function(self):\n",
    "        print('my pipeline')\n",
    "        return ''\n",
    "    \n",
    "# Let's turn our basic pipeline into an implementation of this abstraction\n",
    "class MainPipeline(AbstractPipeline):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.validators.append(ColumnNameValidation())\n",
    "    \n",
    "    # This is our old pipeline implemented in this class now\n",
    "    def pipeline_function(self):\n",
    "        # Imagine a long, complicated pipeline for the rest of the function!!\n",
    "        for idx, row in self.df.iterrows():\n",
    "            # We are just printing the pick name to keep the actual example really simple\n",
    "            print(row['pick_name'])\n",
    "        return 'awesome pipeline outputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok we've changed our pipeline into a 'class' and then made our old pipeline into an implementation of this, let's check that it works:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting pit of despair\n",
      "a_1\n",
      "None\n",
      "AA2\n",
      "bb1\n",
      "AA2\n",
      "starting POC\n",
      "A1\n",
      "B2\n",
      "C2\n",
      "A1\n",
      "B2\n",
      "D1\n"
     ]
    }
   ],
   "source": [
    "# 'pit of despair' asset\n",
    "print('starting pit of despair')\n",
    "pipeline_output = MainPipeline().run(pit_of_despair_picks)\n",
    "print('starting POC')\n",
    "# poc asset\n",
    "pipeline_output = MainPipeline().run(poc_picks_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Now let's extend our old pipeline to use our new validator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewPipeline(MainPipeline):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.validators.append(ApiToIntegerValidator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty quick right!? We just needed to add the new validator and leave the rest untouched, let's try it and see if the well column is now an int:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1\n",
      "B2\n",
      "C2\n",
      "A1\n",
      "B2\n",
      "D1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pick_name</th>\n",
       "      <th>pick_depth</th>\n",
       "      <th>pick_interpreter</th>\n",
       "      <th>well</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>AJ</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B2</td>\n",
       "      <td>150.0</td>\n",
       "      <td>AJ</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C2</td>\n",
       "      <td>400.0</td>\n",
       "      <td>AJ</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1</td>\n",
       "      <td>300.0</td>\n",
       "      <td>AJ</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pick_name  pick_depth pick_interpreter   well\n",
       "0        A1       100.0               AJ  10000\n",
       "1        B2       150.0               AJ  10000\n",
       "2        C2       400.0               AJ  10000\n",
       "3        A1       300.0               AJ  20000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = NewPipeline()\n",
    "pipeline.run(poc_picks_df)\n",
    "pipeline.df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pick_name</th>\n",
       "      <th>pick_depth</th>\n",
       "      <th>pick_interpreter</th>\n",
       "      <th>well</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>AJ</td>\n",
       "      <td>00000000010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B2</td>\n",
       "      <td>150.0</td>\n",
       "      <td>AJ</td>\n",
       "      <td>00000000010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C2</td>\n",
       "      <td>400.0</td>\n",
       "      <td>AJ</td>\n",
       "      <td>00000000010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1</td>\n",
       "      <td>300.0</td>\n",
       "      <td>AJ</td>\n",
       "      <td>00000000020000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pick_name  pick_depth pick_interpreter            well\n",
       "0        A1       100.0               AJ  00000000010000\n",
       "1        B2       150.0               AJ  00000000010000\n",
       "2        C2       400.0               AJ  00000000010000\n",
       "3        A1       300.0               AJ  00000000020000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note that the df well column is now an integer instead of a string\n",
    "\n",
    "# Here is the original (pre-validation df):\n",
    "poc_picks_df = pd.DataFrame({\n",
    "    'pick_name': ['A1', 'B2', 'C2', 'A1', 'B2', 'D1'],\n",
    "    'pick_depth': [100.0, 150.0, 400.0, 300.0, 400.0, 800.0],\n",
    "    'pick_interpreter': ['AJ', 'AJ', 'AJ', 'AJ', 'AJ', 'AJ'],\n",
    "    'well': ['00000000010000', '00000000010000', '00000000010000', '00000000020000', '00000000020000', '00000000020000']\n",
    "})\n",
    "poc_picks_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this illustrates the power and flexibility of the open/closed principle, with some refactoring of the way we ran our pipeline were able to avoid changing anything about our legacy 'deployed' pipeline and still we able to service a new request via writing new implementations and extensions of our existing code and the code was very minimal and focused only on the required changes and without duplicating already implemented code. This example also touches on the interface segregation principle in that we split the implementation of the original pipeline from the implementation of the new customer's request, this important as if too many customers/actors depend on the same code they can have conflicting requirements and cause lots of conflict/technical debt when those conflicts are resolved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
